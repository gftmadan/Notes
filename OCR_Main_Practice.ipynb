{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0900f372-b5e1-453b-9337-1afe38cf2c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytesseract as pyt\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9928357-8caa-4503-a1a8-c63e5e40a3bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Version('5.3.1.20230401')>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyt.get_tesseract_version()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a4f8879-a480-4cae-8d42-c3f0642abebe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage:\n",
      "  tesseract --help | --help-extra | --version\n",
      "  tesseract --list-langs\n",
      "  tesseract imagename outputbase [options...] [configfile...]\n",
      "\n",
      "OCR options:\n",
      "  -l LANG[+LANG]        Specify language(s) used for OCR.\n",
      "NOTE: These options must occur before any configfile.\n",
      "\n",
      "Single options:\n",
      "  --help                Show this help message.\n",
      "  --help-extra          Show extra help for advanced users.\n",
      "  --version             Show version information.\n",
      "  --list-langs          List available languages for tesseract engine.\n"
     ]
    }
   ],
   "source": [
    "!tesseract --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05fa7d2f-f539-42a0-95bb-e9c77ece582a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage:\n",
      "  tesseract --help | --help-extra | --help-psm | --help-oem | --version\n",
      "  tesseract --list-langs [--tessdata-dir PATH]\n",
      "  tesseract --print-fonts-table [options...] [configfile...]\n",
      "  tesseract --print-parameters [options...] [configfile...]\n",
      "  tesseract imagename|imagelist|stdin outputbase|stdout [options...] [configfile...]\n",
      "\n",
      "OCR options:\n",
      "  --tessdata-dir PATH   Specify the location of tessdata path.\n",
      "  --user-words PATH     Specify the location of user words file.\n",
      "  --user-patterns PATH  Specify the location of user patterns file.\n",
      "  --dpi VALUE           Specify DPI for input image.\n",
      "  --loglevel LEVEL      Specify logging level. LEVEL can be\n",
      "                        ALL, TRACE, DEBUG, INFO, WARN, ERROR, FATAL or OFF.\n",
      "  -l LANG[+LANG]        Specify language(s) used for OCR.\n",
      "  -c VAR=VALUE          Set value for config variables.\n",
      "                        Multiple -c arguments are allowed.\n",
      "  --psm NUM             Specify page segmentation mode.\n",
      "  --oem NUM             Specify OCR Engine mode.\n",
      "NOTE: These options must occur before any configfile.\n",
      "\n",
      "Page segmentation modes:\n",
      "  0    Orientation and script detection (OSD) only.\n",
      "  1    Automatic page segmentation with OSD.\n",
      "  2    Automatic page segmentation, but no OSD, or OCR. (not implemented)\n",
      "  3    Fully automatic page segmentation, but no OSD. (Default)\n",
      "  4    Assume a single column of text of variable sizes.\n",
      "  5    Assume a single uniform block of vertically aligned text.\n",
      "  6    Assume a single uniform block of text.\n",
      "  7    Treat the image as a single text line.\n",
      "  8    Treat the image as a single word.\n",
      "  9    Treat the image as a single word in a circle.\n",
      " 10    Treat the image as a single character.\n",
      " 11    Sparse text. Find as much text as possible in no particular order.\n",
      " 12    Sparse text with OSD.\n",
      " 13    Raw line. Treat the image as a single text line,\n",
      "       bypassing hacks that are Tesseract-specific.\n",
      "\n",
      "OCR Engine modes:\n",
      "  0    Legacy engine only.\n",
      "  1    Neural nets LSTM engine only.\n",
      "  2    Legacy + LSTM engines.\n",
      "  3    Default, based on what is available.\n",
      "\n",
      "Single options:\n",
      "  -h, --help            Show minimal help message.\n",
      "  --help-extra          Show extra help for advanced users.\n",
      "  --help-psm            Show page segmentation modes.\n",
      "  --help-oem            Show OCR Engine modes.\n",
      "  -v, --version         Show version information.\n",
      "  --list-langs          List available languages for tesseract engine.\n",
      "  --print-fonts-table   Print tesseract fonts table.\n",
      "  --print-parameters    Print tesseract parameters.\n"
     ]
    }
   ],
   "source": [
    "!tesseract --help-extra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3bec60ab-dfcb-472e-9aff-cac8e6764941",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesseract parameters:\n",
      "log_level\t2147483647\tLogging level\n",
      "textord_dotmatrix_gap\t3\tMax pixel gap for broken pixed pitch\n",
      "textord_debug_block\t0\tBlock to do debug on\n",
      "textord_pitch_range\t2\tMax range test on pitch\n",
      "textord_words_veto_power\t5\tRows required to outvote a veto\n",
      "textord_tabfind_show_strokewidths\t0\tShow stroke widths (ScrollView)\n",
      "pitsync_linear_version\t6\tUse new fast algorithm\n",
      "oldbl_holed_losscount\t10\tMax lost before fallback line used\n",
      "textord_skewsmooth_offset\t4\tFor smooth factor\n",
      "textord_skewsmooth_offset2\t1\tFor smooth factor\n",
      "textord_test_x\t-2147483647\tcoord of test pt\n",
      "textord_test_y\t-2147483647\tcoord of test pt\n",
      "textord_min_blobs_in_row\t4\tMin blobs before gradient counted\n",
      "textord_spline_minblobs\t8\tMin blobs in each spline segment\n",
      "textord_spline_medianwin\t6\tSize of window for spline segmentation\n",
      "textord_max_blob_overlaps\t4\tMax number of blobs a big blob can overlap\n",
      "textord_min_xheight\t10\tMin credible pixel xheight\n",
      "textord_lms_line_trials\t12\tNumber of linew fits to do\n",
      "textord_tabfind_show_images\t0\tShow image blobs\n",
      "textord_fp_chop_error\t2\tMax allowed bending of chop cells\n",
      "edges_max_children_per_outline\t10\tMax number of children inside a character outline\n",
      "edges_max_children_layers\t5\tMax layers of nested children inside a character outline\n",
      "edges_children_per_grandchild\t10\tImportance ratio for chucking outlines\n",
      "edges_children_count_limit\t45\tMax holes allowed in blob\n",
      "edges_min_nonhole\t12\tMin pixels for potential char in box\n",
      "edges_patharea_ratio\t40\tMax lensq/area for acceptable child outline\n",
      "devanagari_split_debuglevel\t0\tDebug level for split shiro-rekha process.\n",
      "textord_tabfind_show_partitions\t0\tShow partition bounds, waiting if >1 (ScrollView)\n",
      "textord_debug_tabfind\t0\tDebug tab finding\n",
      "textord_debug_bugs\t0\tTurn on output related to bugs in tab finding\n",
      "textord_testregion_left\t-1\tLeft edge of debug reporting rectangle in Leptonica coords (bottom=0/top=height), with horizontal lines x/y-flipped\n",
      "textord_testregion_top\t2147483647\tTop edge of debug reporting rectangle in Leptonica coords (bottom=0/top=height), with horizontal lines x/y-flipped\n",
      "textord_testregion_right\t2147483647\tRight edge of debug rectangle in Leptonica coords (bottom=0/top=height), with horizontal lines x/y-flipped\n",
      "textord_testregion_bottom\t-1\tBottom edge of debug rectangle in Leptonica coords (bottom=0/top=height), with horizontal lines x/y-flipped\n",
      "classify_num_cp_levels\t3\tNumber of Class Pruner Levels\n",
      "editor_image_xpos\t590\tEditor image X Pos\n",
      "editor_image_ypos\t10\tEditor image Y Pos\n",
      "editor_image_menuheight\t50\tAdd to image height for menu bar\n",
      "editor_image_word_bb_color\t7\tWord bounding box colour\n",
      "editor_image_blob_bb_color\t4\tBlob bounding box colour\n",
      "editor_word_xpos\t60\tWord window X Pos\n",
      "editor_word_ypos\t510\tWord window Y Pos\n",
      "editor_word_height\t240\tWord window height\n",
      "editor_word_width\t655\tWord window width\n",
      "curl_timeout\t0\tTimeout for curl in seconds\n",
      "wordrec_display_all_blobs\t0\tDisplay Blobs\n",
      "wordrec_blob_pause\t0\tBlob pause\n",
      "textord_force_make_prop_words\t0\tForce proportional word segmentation on all rows\n",
      "textord_chopper_test\t0\tChopper is being tested.\n",
      "textord_restore_underlines\t1\tChop underlines & put back\n",
      "textord_show_initial_words\t0\tDisplay separate words\n",
      "textord_blocksall_fixed\t0\tMoan about prop blocks\n",
      "textord_blocksall_prop\t0\tMoan about fixed pitch blocks\n",
      "textord_pitch_scalebigwords\t0\tScale scores on big words\n",
      "textord_all_prop\t0\tAll doc is proportial text\n",
      "textord_debug_pitch_test\t0\tDebug on fixed pitch test\n",
      "textord_disable_pitch_test\t0\tTurn off dp fixed pitch algorithm\n",
      "textord_fast_pitch_test\t0\tDo even faster pitch algorithm\n",
      "textord_debug_pitch_metric\t0\tWrite full metric stuff\n",
      "textord_show_row_cuts\t0\tDraw row-level cuts\n",
      "textord_show_page_cuts\t0\tDraw page-level cuts\n",
      "textord_blockndoc_fixed\t0\tAttempt whole doc/block fixed pitch\n",
      "textord_show_tables\t0\tShow table regions (ScrollView)\n",
      "textord_tablefind_show_mark\t0\tDebug table marking steps in detail (ScrollView)\n",
      "textord_tablefind_show_stats\t0\tShow page stats used in table finding (ScrollView)\n",
      "textord_tablefind_recognize_tables\t0\tEnables the table recognizer for table layout and filtering.\n",
      "textord_tabfind_show_initialtabs\t0\tShow tab candidates\n",
      "textord_tabfind_show_finaltabs\t0\tShow tab vectors\n",
      "textord_tabfind_only_strokewidths\t0\tOnly run stroke widths\n",
      "textord_really_old_xheight\t0\tUse original wiseowl xheight\n",
      "textord_oldbl_debug\t0\tDebug old baseline generation\n",
      "textord_debug_baselines\t0\tDebug baseline generation\n",
      "textord_oldbl_paradef\t1\tUse para default mechanism\n",
      "textord_oldbl_split_splines\t1\tSplit stepped splines\n",
      "textord_oldbl_merge_parts\t1\tMerge suspect partitions\n",
      "oldbl_corrfix\t1\tImprove correlation of heights\n",
      "oldbl_xhfix\t0\tFix bug in modes threshold for xheights\n",
      "textord_ocropus_mode\t0\tMake baselines for ocropus\n",
      "textord_heavy_nr\t0\tVigorously remove noise\n",
      "textord_show_initial_rows\t0\tDisplay row accumulation\n",
      "textord_show_parallel_rows\t0\tDisplay page correlated rows\n",
      "textord_show_expanded_rows\t0\tDisplay rows after expanding\n",
      "textord_show_final_rows\t0\tDisplay rows after final fitting\n",
      "textord_show_final_blobs\t0\tDisplay blob bounds after pre-ass\n",
      "textord_test_landscape\t0\tTests refer to land/port\n",
      "textord_parallel_baselines\t1\tForce parallel baselines\n",
      "textord_straight_baselines\t0\tForce straight baselines\n",
      "textord_old_baselines\t1\tUse old baseline algorithm\n",
      "textord_old_xheight\t0\tUse old xheight algorithm\n",
      "textord_fix_xheight_bug\t1\tUse spline baseline\n",
      "textord_fix_makerow_bug\t1\tPrevent multiple baselines\n",
      "textord_debug_xheights\t0\tTest xheight algorithms\n",
      "textord_biased_skewcalc\t1\tBias skew estimates with line length\n",
      "textord_interpolating_skew\t1\tInterpolate across gaps\n",
      "textord_new_initial_xheight\t1\tUse test xheight mechanism\n",
      "textord_debug_blob\t0\tPrint test blob information\n",
      "gapmap_debug\t0\tSay which blocks have tables\n",
      "gapmap_use_ends\t0\tUse large space at start and end of rows\n",
      "gapmap_no_isolated_quanta\t0\tEnsure gaps not less than 2quanta wide\n",
      "edges_use_new_outline_complexity\t0\tUse the new outline complexity module\n",
      "edges_debug\t0\tturn on debugging for this module\n",
      "edges_children_fix\t0\tRemove boxy parents of char-like children\n",
      "textord_show_fixed_cuts\t0\tDraw fixed pitch cell boundaries\n",
      "devanagari_split_debugimage\t0\tWhether to create a debug image for split shiro-rekha process.\n",
      "textord_tabfind_show_initial_partitions\t0\tShow partition bounds\n",
      "textord_tabfind_show_reject_blobs\t0\tShow blobs rejected as noise\n",
      "textord_tabfind_show_columns\t0\tShow column bounds (ScrollView)\n",
      "textord_tabfind_show_blocks\t0\tShow final block bounds (ScrollView)\n",
      "textord_tabfind_find_tables\t1\trun table detection\n",
      "textord_space_size_is_variable\t0\tIf true, word delimiter spaces are assumed to have variable width, even though characters have fixed pitch.\n",
      "textord_debug_printable\t0\tMake debug windows printable\n",
      "wordrec_display_splits\t0\tDisplay splits\n",
      "poly_debug\t0\tDebug old poly\n",
      "poly_wide_objects_better\t1\tMore accurate approx on wide things\n",
      "equationdetect_save_bi_image\t0\tSave input bi image\n",
      "equationdetect_save_spt_image\t0\tSave special character image\n",
      "equationdetect_save_seed_image\t0\tSave the seed image\n",
      "equationdetect_save_merged_image\t0\tSave the merged image\n",
      "stream_filelist\t0\tStream a filelist from stdin\n",
      "debug_file\t\tFile to send tprintf output to\n",
      "editor_image_win_name\tEditorImage\tEditor image window name\n",
      "editor_word_name\tBlnWords\tBL normalized word window\n",
      "dotproduct\tgeneric\tFunction used for calculation of dot product\n",
      "document_title\t\tTitle of output document (used for hOCR and PDF output)\n",
      "classify_font_name\tUnknownFont\tDefault font name to be used in training\n",
      "textord_underline_offset\t0.1\tFraction of x to ignore\n",
      "textord_wordstats_smooth_factor\t0.05\tSmoothing gap stats\n",
      "textord_words_maxspace\t4\tMultiple of xheight\n",
      "textord_words_default_maxspace\t3.5\tMax believable third space\n",
      "textord_words_default_minspace\t0.6\tFraction of xheight\n",
      "textord_words_min_minspace\t0.3\tFraction of xheight\n",
      "textord_words_default_nonspace\t0.2\tFraction of xheight\n",
      "textord_words_initial_lower\t0.25\tMax initial cluster size\n",
      "textord_words_initial_upper\t0.15\tMin initial cluster spacing\n",
      "textord_words_minlarge\t0.75\tFraction of valid gaps needed\n",
      "textord_words_pitchsd_threshold\t0.04\tPitch sync threshold\n",
      "textord_words_def_fixed\t0.016\tThreshold for definite fixed\n",
      "textord_words_def_prop\t0.09\tThreshold for definite prop\n",
      "textord_pitch_rowsimilarity\t0.08\tFraction of xheight for sameness\n",
      "words_initial_lower\t0.5\tMax initial cluster size\n",
      "words_initial_upper\t0.15\tMin initial cluster spacing\n",
      "words_default_prop_nonspace\t0.25\tFraction of xheight\n",
      "words_default_fixed_space\t0.75\tFraction of xheight\n",
      "words_default_fixed_limit\t0.6\tAllowed size variance\n",
      "textord_words_definite_spread\t0.3\tNon-fuzzy spacing region\n",
      "textord_spacesize_ratioprop\t2\tMin ratio space/nonspace\n",
      "textord_fpiqr_ratio\t1.5\tPitch IQR/Gap IQR threshold\n",
      "textord_max_pitch_iqr\t0.2\tXh fraction noise in pitch\n",
      "textord_projection_scale\t0.2\tDing rate for mid-cuts\n",
      "textord_balance_factor\t1\tDing rate for unbalanced char cells\n",
      "textord_tabvector_vertical_gap_fraction\t0.5\tmax fraction of mean blob width allowed for vertical gaps in vertical text\n",
      "textord_tabvector_vertical_box_ratio\t0.5\tFraction of box matches required to declare a line vertical\n",
      "pitsync_joined_edge\t0.75\tDist inside big blob for chopping\n",
      "pitsync_offset_freecut_fraction\t0.25\tFraction of cut for free cuts\n",
      "oldbl_xhfract\t0.4\tFraction of est allowed in calc\n",
      "oldbl_dot_error_size\t1.26\tMax aspect ratio of a dot\n",
      "textord_oldbl_jumplimit\t0.15\tX fraction for new partition\n",
      "textord_spline_shift_fraction\t0.02\tFraction of line spacing for quad\n",
      "textord_skew_ile\t0.5\tIle of gradients for page skew\n",
      "textord_skew_lag\t0.02\tLag for skew on row accumulation\n",
      "textord_linespace_iqrlimit\t0.2\tMax iqr/median for linespace\n",
      "textord_width_limit\t8\tMax width of blobs to make rows\n",
      "textord_chop_width\t1.5\tMax width before chopping\n",
      "textord_expansion_factor\t1\tFactor to expand rows by in expand_rows\n",
      "textord_overlap_x\t0.375\tFraction of linespace for good overlap\n",
      "textord_minxh\t0.25\tfraction of linesize for min xheight\n",
      "textord_min_linesize\t1.25\t* blob height for initial linesize\n",
      "textord_excess_blobsize\t1.3\tNew row made if blob makes row this big\n",
      "textord_occupancy_threshold\t0.4\tFraction of neighbourhood\n",
      "textord_underline_width\t2\tMultiple of line_size for underline\n",
      "textord_min_blob_height_fraction\t0.75\tMin blob height/top to include blob top into xheight stats\n",
      "textord_xheight_mode_fraction\t0.4\tMin pile height to make xheight\n",
      "textord_ascheight_mode_fraction\t0.08\tMin pile height to make ascheight\n",
      "textord_descheight_mode_fraction\t0.08\tMin pile height to make descheight\n",
      "textord_ascx_ratio_min\t1.25\tMin cap/xheight\n",
      "textord_ascx_ratio_max\t1.8\tMax cap/xheight\n",
      "textord_descx_ratio_min\t0.25\tMin desc/xheight\n",
      "textord_descx_ratio_max\t0.6\tMax desc/xheight\n",
      "textord_xheight_error_margin\t0.1\tAccepted variation\n",
      "gapmap_big_gaps\t1.75\txht multiplier\n",
      "edges_childarea\t0.5\tMin area fraction of child outline\n",
      "edges_boxarea\t0.875\tMin area fraction of grandchild for box\n",
      "textord_underline_threshold\t0.5\tFraction of width occupied\n",
      "classify_pico_feature_length\t0.05\tPico Feature Length\n",
      "classify_norm_adj_midpoint\t32\tNorm adjust midpoint ...\n",
      "classify_norm_adj_curl\t2\tNorm adjust curl ...\n",
      "classify_min_slope\t0.414214\tSlope below which lines are called horizontal\n",
      "classify_max_slope\t2.41421\tSlope above which lines are called vertical\n",
      "classify_cp_angle_pad_loose\t45\tClass Pruner Angle Pad Loose\n",
      "classify_cp_angle_pad_medium\t20\tClass Pruner Angle Pad Medium\n",
      "classify_cp_angle_pad_tight\t10\tCLass Pruner Angle Pad Tight\n",
      "classify_cp_end_pad_loose\t0.5\tClass Pruner End Pad Loose\n",
      "classify_cp_end_pad_medium\t0.5\tClass Pruner End Pad Medium\n",
      "classify_cp_end_pad_tight\t0.5\tClass Pruner End Pad Tight\n",
      "classify_cp_side_pad_loose\t2.5\tClass Pruner Side Pad Loose\n",
      "classify_cp_side_pad_medium\t1.2\tClass Pruner Side Pad Medium\n",
      "classify_cp_side_pad_tight\t0.6\tClass Pruner Side Pad Tight\n",
      "classify_pp_angle_pad\t45\tProto Pruner Angle Pad\n",
      "classify_pp_end_pad\t0.5\tProto Prune End Pad\n",
      "classify_pp_side_pad\t2.5\tProto Pruner Side Pad\n",
      "ambigs_debug_level\t0\tDebug level for unichar ambiguities\n",
      "classify_debug_level\t0\tClassify debug level\n",
      "classify_norm_method\t1\tNormalization Method   ...\n",
      "matcher_debug_level\t0\tMatcher Debug Level\n",
      "matcher_debug_flags\t0\tMatcher Debug Flags\n",
      "classify_learning_debug_level\t0\tLearning Debug Level: \n",
      "matcher_permanent_classes_min\t1\tMin # of permanent classes\n",
      "matcher_min_examples_for_prototyping\t3\tReliable Config Threshold\n",
      "matcher_sufficient_examples_for_prototyping\t5\tEnable adaption even if the ambiguities have not been seen\n",
      "classify_adapt_proto_threshold\t230\tThreshold for good protos during adaptive 0-255\n",
      "classify_adapt_feature_threshold\t230\tThreshold for good features during adaptive 0-255\n",
      "classify_class_pruner_threshold\t229\tClass Pruner Threshold 0-255\n",
      "classify_class_pruner_multiplier\t15\tClass Pruner Multiplier 0-255:       \n",
      "classify_cp_cutoff_strength\t7\tClass Pruner CutoffStrength:         \n",
      "classify_integer_matcher_multiplier\t10\tInteger Matcher Multiplier  0-255:   \n",
      "dawg_debug_level\t0\tSet to 1 for general debug info, to 2 for more details, to 3 to see all the debug messages\n",
      "hyphen_debug_level\t0\tDebug level for hyphenated words.\n",
      "stopper_smallword_size\t2\tSize of dict word to be treated as non-dict word\n",
      "stopper_debug_level\t0\tStopper debug level\n",
      "tessedit_truncate_wordchoice_log\t10\tMax words to keep in list\n",
      "max_permuter_attempts\t10000\tMaximum number of different character choices to consider during permutation. This limit is especially useful when user patterns are specified, since overly generic patterns can result in dawg search exploring an overly large number of options.\n",
      "repair_unchopped_blobs\t1\tFix blobs that aren't chopped\n",
      "chop_debug\t0\tChop debug\n",
      "chop_split_length\t10000\tSplit Length\n",
      "chop_same_distance\t2\tSame distance\n",
      "chop_min_outline_points\t6\tMin Number of Points on Outline\n",
      "chop_seam_pile_size\t150\tMax number of seams in seam_pile\n",
      "chop_inside_angle\t-50\tMin Inside Angle Bend\n",
      "chop_min_outline_area\t2000\tMin Outline Area\n",
      "chop_centered_maxwidth\t90\tWidth of (smaller) chopped blobs above which we don't care that a chop is not near the center.\n",
      "chop_x_y_weight\t3\tX / Y  length weight\n",
      "wordrec_debug_level\t0\tDebug level for wordrec\n",
      "wordrec_max_join_chunks\t4\tMax number of broken pieces to associate\n",
      "segsearch_debug_level\t0\tSegSearch debug level\n",
      "segsearch_max_pain_points\t2000\tMaximum number of pain points stored in the queue\n",
      "segsearch_max_futile_classifications\t20\tMaximum number of pain point classifications per chunk that did not result in finding a better word choice.\n",
      "language_model_debug_level\t0\tLanguage model debug level\n",
      "language_model_ngram_order\t8\tMaximum order of the character ngram model\n",
      "language_model_viterbi_list_max_num_prunable\t10\tMaximum number of prunable (those for which PrunablePath() is true) entries in each viterbi list recorded in BLOB_CHOICEs\n",
      "language_model_viterbi_list_max_size\t500\tMaximum size of viterbi lists recorded in BLOB_CHOICEs\n",
      "language_model_min_compound_length\t3\tMinimum length of compound words\n",
      "wordrec_display_segmentations\t0\tDisplay Segmentations (ScrollView)\n",
      "tessedit_pageseg_mode\t6\tPage seg mode: 0=osd only, 1=auto+osd, 2=auto_only, 3=auto, 4=column, 5=block_vert, 6=block, 7=line, 8=word, 9=word_circle, 10=char,11=sparse_text, 12=sparse_text+osd, 13=raw_line (Values from PageSegMode enum in tesseract/publictypes.h)\n",
      "thresholding_method\t0\tThresholding method: 0 = Otsu, 1 = LeptonicaOtsu, 2 = Sauvola\n",
      "tessedit_ocr_engine_mode\t3\tWhich OCR engine(s) to run (Tesseract, LSTM, both). Defaults to loading and running the most accurate available.\n",
      "pageseg_devanagari_split_strategy\t0\tWhether to use the top-line splitting process for Devanagari documents while performing page-segmentation.\n",
      "ocr_devanagari_split_strategy\t0\tWhether to use the top-line splitting process for Devanagari documents while performing ocr.\n",
      "bidi_debug\t0\tDebug level for BiDi\n",
      "applybox_debug\t1\tDebug level\n",
      "applybox_page\t0\tPage number to apply boxes from\n",
      "tessedit_font_id\t0\tFont ID to use or zero\n",
      "tessedit_bigram_debug\t0\tAmount of debug output for bigram correction.\n",
      "debug_noise_removal\t0\tDebug reassignment of small outlines\n",
      "noise_maxperblob\t8\tMax diacritics to apply to a blob\n",
      "noise_maxperword\t16\tMax diacritics to apply to a word\n",
      "debug_x_ht_level\t0\tReestimate debug\n",
      "quality_min_initial_alphas_reqd\t2\talphas in a good word\n",
      "tessedit_tess_adaption_mode\t39\tAdaptation decision algorithm for tess\n",
      "multilang_debug_level\t0\tPrint multilang debug info.\n",
      "paragraph_debug_level\t0\tPrint paragraph debug info.\n",
      "tessedit_preserve_min_wd_len\t2\tOnly preserve wds longer than this\n",
      "crunch_rating_max\t10\tFor adj length in rating per ch\n",
      "crunch_pot_indicators\t1\tHow many potential indicators needed\n",
      "crunch_leave_lc_strings\t4\tDon't crunch words with long lower case strings\n",
      "crunch_leave_uc_strings\t4\tDon't crunch words with long lower case strings\n",
      "crunch_long_repetitions\t3\tCrunch words with long repetitions\n",
      "crunch_debug\t0\tAs it says\n",
      "fixsp_non_noise_limit\t1\tHow many non-noise blbs either side?\n",
      "fixsp_done_mode\t1\tWhat constitutes done for spacing\n",
      "debug_fix_space_level\t0\tContextual fixspace debug\n",
      "x_ht_acceptance_tolerance\t8\tMax allowed deviation of blob top outside of font data\n",
      "x_ht_min_change\t8\tMin change in xht before actually trying it\n",
      "superscript_debug\t0\tDebug level for sub & superscript fixer\n",
      "jpg_quality\t85\tSet JPEG quality level\n",
      "user_defined_dpi\t0\tSpecify DPI for input image\n",
      "min_characters_to_try\t50\tSpecify minimum characters to try during OSD\n",
      "suspect_level\t99\tSuspect marker level\n",
      "suspect_short_words\t2\tDon't suspect dict wds longer than this\n",
      "tessedit_reject_mode\t0\tRejection algorithm\n",
      "tessedit_image_border\t2\tRej blbs near image edge limit\n",
      "min_sane_x_ht_pixels\t8\tReject any x-ht lt or eq than this\n",
      "tessedit_page_number\t-1\t-1 -> All pages, else specific page to process\n",
      "tessedit_parallelize\t0\tRun in parallel where possible\n",
      "lstm_choice_mode\t0\tAllows to include alternative symbols choices in the hOCR output. Valid input values are 0, 1 and 2. 0 is the default value. With 1 the alternative symbol choices per timestep are included. With 2 alternative symbol choices are extracted from the CTC process instead of the lattice. The choices are mapped per character.\n",
      "lstm_choice_iterations\t5\tSets the number of cascading iterations for the Beamsearch in lstm_choice_mode. Note that lstm_choice_mode must be set to a value greater than 0 to produce results.\n",
      "tosp_debug_level\t0\tDebug data\n",
      "tosp_enough_space_samples_for_median\t3\tor should we use mean\n",
      "tosp_redo_kern_limit\t10\tNo.samples reqd to reestimate for row\n",
      "tosp_few_samples\t40\tNo.gaps reqd with 1 large gap to treat as a table\n",
      "tosp_short_row\t20\tNo.gaps reqd with few cert spaces to use certs\n",
      "tosp_sanity_method\t1\tHow to avoid being silly\n",
      "textord_max_noise_size\t7\tPixel size of noise\n",
      "textord_baseline_debug\t0\tBaseline debug level\n",
      "textord_noise_sizefraction\t10\tFraction of size for maxima\n",
      "textord_noise_translimit\t16\tTransitions for normal blob\n",
      "textord_noise_sncount\t1\tsuper norm blobs to save row\n",
      "use_ambigs_for_adaption\t0\tUse ambigs for deciding whether to adapt to a character\n",
      "allow_blob_division\t1\tUse divisible blobs chopping\n",
      "prioritize_division\t0\tPrioritize blob division over chopping\n",
      "classify_enable_learning\t1\tEnable adaptive classifier\n",
      "tess_cn_matching\t0\tCharacter Normalized Matching\n",
      "tess_bn_matching\t0\tBaseline Normalized Matching\n",
      "classify_enable_adaptive_matcher\t1\tEnable adaptive classifier\n",
      "classify_use_pre_adapted_templates\t0\tUse pre-adapted classifier templates\n",
      "classify_save_adapted_templates\t0\tSave adapted templates to a file\n",
      "classify_enable_adaptive_debugger\t0\tEnable match debugger\n",
      "classify_nonlinear_norm\t0\tNon-linear stroke-density normalization\n",
      "disable_character_fragments\t1\tDo not include character fragments in the results of the classifier\n",
      "classify_debug_character_fragments\t0\tBring up graphical debugging windows for fragments training\n",
      "matcher_debug_separate_windows\t0\tUse two different windows for debugging the matching: One for the protos and one for the features.\n",
      "classify_bln_numeric_mode\t0\tAssume the input is numbers [0-9].\n",
      "load_system_dawg\t1\tLoad system word dawg.\n",
      "load_freq_dawg\t1\tLoad frequent word dawg.\n",
      "load_unambig_dawg\t1\tLoad unambiguous word dawg.\n",
      "load_punc_dawg\t1\tLoad dawg with punctuation patterns.\n",
      "load_number_dawg\t1\tLoad dawg with number patterns.\n",
      "load_bigram_dawg\t1\tLoad dawg with special word bigrams.\n",
      "use_only_first_uft8_step\t0\tUse only the first UTF8 step of the given string when computing log probabilities.\n",
      "stopper_no_acceptable_choices\t0\tMake AcceptableChoice() always return false. Useful when there is a need to explore all segmentations\n",
      "segment_nonalphabetic_script\t0\tDon't use any alphabetic-specific tricks. Set to true in the traineddata config file for scripts that are cursive or inherently fixed-pitch\n",
      "save_doc_words\t0\tSave Document Words\n",
      "merge_fragments_in_matrix\t1\tMerge the fragments in the ratings matrix and delete them after merging\n",
      "wordrec_enable_assoc\t1\tAssociator Enable\n",
      "force_word_assoc\t0\tforce associator to run regardless of what enable_assoc is. This is used for CJK where component grouping is necessary.\n",
      "chop_enable\t1\tChop enable\n",
      "chop_vertical_creep\t0\tVertical creep\n",
      "chop_new_seam_pile\t1\tUse new seam_pile\n",
      "assume_fixed_pitch_char_segment\t0\tinclude fixed-pitch heuristics in char segmentation\n",
      "wordrec_skip_no_truth_words\t0\tOnly run OCR for words that had truth recorded in BlamerBundle\n",
      "wordrec_debug_blamer\t0\tPrint blamer debug messages\n",
      "wordrec_run_blamer\t0\tTry to set the blame for errors\n",
      "save_alt_choices\t1\tSave alternative paths found during chopping and segmentation search\n",
      "language_model_ngram_on\t0\tTurn on/off the use of character ngram model\n",
      "language_model_ngram_use_only_first_uft8_step\t0\tUse only the first UTF8 step of the given string when computing log probabilities.\n",
      "language_model_ngram_space_delimited_language\t1\tWords are delimited by space\n",
      "language_model_use_sigmoidal_certainty\t0\tUse sigmoidal score for certainty\n",
      "tessedit_resegment_from_boxes\t0\tTake segmentation and labeling from box file\n",
      "tessedit_resegment_from_line_boxes\t0\tConversion of word/line box file to char box file\n",
      "tessedit_train_from_boxes\t0\tGenerate training data from boxed chars\n",
      "tessedit_make_boxes_from_boxes\t0\tGenerate more boxes from boxed chars\n",
      "tessedit_train_line_recognizer\t0\tBreak input into lines and remap boxes if present\n",
      "tessedit_dump_pageseg_images\t0\tDump intermediate images made during page segmentation\n",
      "tessedit_do_invert\t1\tTry inverted line image if necessary (deprecated, will be removed in release 6, use the 'invert_threshold' parameter instead)\n",
      "thresholding_debug\t0\tDebug the thresholding process\n",
      "tessedit_ambigs_training\t0\tPerform training for ambiguities\n",
      "tessedit_adaption_debug\t0\tGenerate and print debug information for adaption\n",
      "applybox_learn_chars_and_char_frags_mode\t0\tLearn both character fragments (as is done in the special low exposure mode) as well as unfragmented characters.\n",
      "applybox_learn_ngrams_mode\t0\tEach bounding box is assumed to contain ngrams. Only learn the ngrams whose outlines overlap horizontally.\n",
      "tessedit_display_outwords\t0\tDraw output words\n",
      "tessedit_dump_choices\t0\tDump char choices\n",
      "tessedit_timing_debug\t0\tPrint timing stats\n",
      "tessedit_fix_fuzzy_spaces\t1\tTry to improve fuzzy spaces\n",
      "tessedit_unrej_any_wd\t0\tDon't bother with word plausibility\n",
      "tessedit_fix_hyphens\t1\tCrunch double hyphens?\n",
      "tessedit_enable_doc_dict\t1\tAdd words to the document dictionary\n",
      "tessedit_debug_fonts\t0\tOutput font info per char\n",
      "tessedit_debug_block_rejection\t0\tBlock and Row stats\n",
      "tessedit_enable_bigram_correction\t1\tEnable correction based on the word bigram dictionary.\n",
      "tessedit_enable_dict_correction\t0\tEnable single word correction based on the dictionary.\n",
      "enable_noise_removal\t1\tRemove and conditionally reassign small outlines when they confuse layout analysis, determining diacritics vs noise\n",
      "tessedit_minimal_rej_pass1\t0\tDo minimal rejection on pass 1 output\n",
      "tessedit_test_adaption\t0\tTest adaption criteria\n",
      "test_pt\t0\tTest for point\n",
      "paragraph_text_based\t1\tRun paragraph detection on the post-text-recognition (more accurate)\n",
      "lstm_use_matrix\t1\tUse ratings matrix/beam search with lstm\n",
      "tessedit_good_quality_unrej\t1\tReduce rejection on good docs\n",
      "tessedit_use_reject_spaces\t1\tReject spaces?\n",
      "tessedit_preserve_blk_rej_perfect_wds\t1\tOnly rej partially rejected words in block rejection\n",
      "tessedit_preserve_row_rej_perfect_wds\t1\tOnly rej partially rejected words in row rejection\n",
      "tessedit_dont_blkrej_good_wds\t0\tUse word segmentation quality metric\n",
      "tessedit_dont_rowrej_good_wds\t0\tUse word segmentation quality metric\n",
      "tessedit_row_rej_good_docs\t1\tApply row rejection to good docs\n",
      "tessedit_reject_bad_qual_wds\t1\tReject all bad quality wds\n",
      "tessedit_debug_doc_rejection\t0\tPage stats\n",
      "tessedit_debug_quality_metrics\t0\tOutput data to debug file\n",
      "bland_unrej\t0\tunrej potential with no checks\n",
      "unlv_tilde_crunching\t0\tMark v.bad words for tilde crunch\n",
      "hocr_font_info\t0\tAdd font info to hocr output\n",
      "hocr_char_boxes\t0\tAdd coordinates for each character to hocr output\n",
      "crunch_early_merge_tess_fails\t1\tBefore word crunch?\n",
      "crunch_early_convert_bad_unlv_chs\t0\tTake out ~^ early?\n",
      "crunch_terrible_garbage\t1\tAs it says\n",
      "crunch_leave_ok_strings\t1\tDon't touch sensible strings\n",
      "crunch_accept_ok\t1\tUse acceptability in okstring\n",
      "crunch_leave_accept_strings\t0\tDon't pot crunch sensible strings\n",
      "crunch_include_numerals\t0\tFiddle alpha figures\n",
      "tessedit_prefer_joined_punct\t0\tReward punctuation joins\n",
      "tessedit_write_block_separators\t0\tWrite block separators in output\n",
      "tessedit_write_rep_codes\t0\tWrite repetition char code\n",
      "tessedit_write_unlv\t0\tWrite .unlv output file\n",
      "tessedit_create_txt\t0\tWrite .txt output file\n",
      "tessedit_create_hocr\t0\tWrite .html hOCR output file\n",
      "tessedit_create_alto\t0\tWrite .xml ALTO file\n",
      "tessedit_create_lstmbox\t0\tWrite .box file for LSTM training\n",
      "tessedit_create_tsv\t0\tWrite .tsv output file\n",
      "tessedit_create_wordstrbox\t0\tWrite WordStr format .box output file\n",
      "tessedit_create_pdf\t0\tWrite .pdf output file\n",
      "textonly_pdf\t0\tCreate PDF with only one invisible text layer\n",
      "suspect_constrain_1Il\t0\tUNLV keep 1Il chars rejected\n",
      "tessedit_minimal_rejection\t0\tOnly reject tess failures\n",
      "tessedit_zero_rejection\t0\tDon't reject ANYTHING\n",
      "tessedit_word_for_word\t0\tMake output have exactly one word per WERD\n",
      "tessedit_zero_kelvin_rejection\t0\tDon't reject ANYTHING AT ALL\n",
      "tessedit_rejection_debug\t0\tAdaption debug\n",
      "tessedit_flip_0O\t1\tContextual 0O O0 flips\n",
      "rej_trust_doc_dawg\t0\tUse DOC dawg in 11l conf. detector\n",
      "rej_1Il_use_dict_word\t0\tUse dictword test\n",
      "rej_1Il_trust_permuter_type\t1\tDon't double check\n",
      "rej_use_tess_accepted\t1\tIndividual rejection control\n",
      "rej_use_tess_blanks\t1\tIndividual rejection control\n",
      "rej_use_good_perm\t1\tIndividual rejection control\n",
      "rej_use_sensible_wd\t0\tExtend permuter check\n",
      "rej_alphas_in_number_perm\t0\tExtend permuter check\n",
      "tessedit_create_boxfile\t0\tOutput text with boxes\n",
      "tessedit_write_images\t0\tCapture the image from the IPE\n",
      "interactive_display_mode\t0\tRun interactively?\n",
      "tessedit_override_permuter\t1\tAccording to dict_word\n",
      "tessedit_use_primary_params_model\t0\tIn multilingual mode use params model of the primary language\n",
      "textord_tabfind_show_vlines\t0\tDebug line finding\n",
      "textord_use_cjk_fp_model\t0\tUse CJK fixed pitch model\n",
      "poly_allow_detailed_fx\t0\tAllow feature extractors to see the original outline\n",
      "tessedit_init_config_only\t0\tOnly initialize with the config file. Useful if the instance is not going to be used for OCR but say only for layout analysis.\n",
      "textord_equation_detect\t0\tTurn on equation detector\n",
      "textord_tabfind_vertical_text\t1\tEnable vertical detection\n",
      "textord_tabfind_force_vertical_text\t0\tForce using vertical text page mode\n",
      "preserve_interword_spaces\t0\tPreserve multiple interword spaces\n",
      "pageseg_apply_music_mask\t0\tDetect music staff and remove intersecting components\n",
      "textord_single_height_mode\t0\tScript has no xheight, so use a single mode\n",
      "tosp_old_to_method\t0\tSpace stats use prechopping?\n",
      "tosp_old_to_constrain_sp_kn\t0\tConstrain relative values of inter and intra-word gaps for old_to_method.\n",
      "tosp_only_use_prop_rows\t1\tBlock stats to use fixed pitch rows?\n",
      "tosp_force_wordbreak_on_punct\t0\tForce word breaks on punct to break long lines in non-space delimited langs\n",
      "tosp_use_pre_chopping\t0\tSpace stats use prechopping?\n",
      "tosp_old_to_bug_fix\t0\tFix suspected bug in old code\n",
      "tosp_block_use_cert_spaces\t1\tOnly stat OBVIOUS spaces\n",
      "tosp_row_use_cert_spaces\t1\tOnly stat OBVIOUS spaces\n",
      "tosp_narrow_blobs_not_cert\t1\tOnly stat OBVIOUS spaces\n",
      "tosp_row_use_cert_spaces1\t1\tOnly stat OBVIOUS spaces\n",
      "tosp_recovery_isolated_row_stats\t1\tUse row alone when inadequate cert spaces\n",
      "tosp_only_small_gaps_for_kern\t0\tBetter guess\n",
      "tosp_all_flips_fuzzy\t0\tPass ANY flip to context?\n",
      "tosp_fuzzy_limit_all\t1\tDon't restrict kn->sp fuzzy limit to tables\n",
      "tosp_stats_use_xht_gaps\t1\tUse within xht gap for wd breaks\n",
      "tosp_use_xht_gaps\t1\tUse within xht gap for wd breaks\n",
      "tosp_only_use_xht_gaps\t0\tOnly use within xht gap for wd breaks\n",
      "tosp_rule_9_test_punct\t0\tDon't chng kn to space next to punct\n",
      "tosp_flip_fuzz_kn_to_sp\t1\tDefault flip\n",
      "tosp_flip_fuzz_sp_to_kn\t1\tDefault flip\n",
      "tosp_improve_thresh\t0\tEnable improvement heuristic\n",
      "textord_no_rejects\t0\tDon't remove noise blobs\n",
      "textord_show_blobs\t0\tDisplay unsorted blobs\n",
      "textord_show_boxes\t0\tDisplay unsorted blobs\n",
      "textord_noise_rejwords\t1\tReject noise-like words\n",
      "textord_noise_rejrows\t1\tReject noise-like rows\n",
      "textord_noise_debug\t0\tDebug row garbage detector\n",
      "classify_learn_debug_str\t\tClass str to debug learning\n",
      "user_words_file\t\tA filename of user-provided words.\n",
      "user_words_suffix\t\tA suffix of user-provided words located in tessdata.\n",
      "user_patterns_file\t\tA filename of user-provided patterns.\n",
      "user_patterns_suffix\t\tA suffix of user-provided patterns located in tessdata.\n",
      "output_ambig_words_file\t\tOutput file for ambiguities found in the dictionary\n",
      "word_to_debug\t\tWord for which stopper debug information should be printed to stdout\n",
      "tessedit_char_blacklist\t\tBlacklist of chars not to recognize\n",
      "tessedit_char_whitelist\t\tWhitelist of chars to recognize\n",
      "tessedit_char_unblacklist\t\tList of chars to override tessedit_char_blacklist\n",
      "tessedit_write_params_to_file\t\tWrite all parameters to the given file.\n",
      "applybox_exposure_pattern\t.exp\tExposure value follows this pattern in the image filename. The name of the image files are expected to be in the form [lang].[fontname].exp[num].tif\n",
      "chs_leading_punct\t('`\"\tLeading punctuation\n",
      "chs_trailing_punct1\t).,;:?!\t1st Trailing punctuation\n",
      "chs_trailing_punct2\t)'`\"\t2nd Trailing punctuation\n",
      "outlines_odd\t%| \tNon standard number of outlines\n",
      "outlines_2\tij!?%\":;\tNon standard number of outlines\n",
      "numeric_punctuation\t.,\tPunct. chs expected WITHIN numbers\n",
      "unrecognised_char\t|\tOutput char for unidentified blobs\n",
      "ok_repeated_ch_non_alphanum_wds\t-?*=\tAllow NN to unrej\n",
      "conflict_set_I_l_1\tIl1[]\tIl1 conflict set\n",
      "file_type\t.tif\tFilename extension\n",
      "tessedit_load_sublangs\t\tList of languages to load with this one\n",
      "page_separator\t\f",
      "\tPage separator (default is form feed control character)\n",
      "classify_char_norm_range\t0.2\tCharacter Normalization Range ...\n",
      "classify_max_rating_ratio\t1.5\tVeto ratio between classifier ratings\n",
      "classify_max_certainty_margin\t5.5\tVeto difference between classifier certainties\n",
      "matcher_good_threshold\t0.125\tGood Match (0-1)\n",
      "matcher_reliable_adaptive_result\t0\tGreat Match (0-1)\n",
      "matcher_perfect_threshold\t0.02\tPerfect Match (0-1)\n",
      "matcher_bad_match_pad\t0.15\tBad Match Pad (0-1)\n",
      "matcher_rating_margin\t0.1\tNew template margin (0-1)\n",
      "matcher_avg_noise_size\t12\tAvg. noise blob length\n",
      "matcher_clustering_max_angle_delta\t0.015\tMaximum angle delta for prototype clustering\n",
      "classify_misfit_junk_penalty\t0\tPenalty to apply when a non-alnum is vertically out of its expected textline position\n",
      "rating_scale\t1.5\tRating scaling factor\n",
      "tessedit_class_miss_scale\t0.00390625\tScale factor for features not used\n",
      "classify_adapted_pruning_factor\t2.5\tPrune poor adapted results this much worse than best result\n",
      "classify_adapted_pruning_threshold\t-1\tThreshold at which classify_adapted_pruning_factor starts\n",
      "classify_character_fragments_garbage_certainty_threshold\t-3\tExclude fragments that do not look like whole characters from training and adaption\n",
      "speckle_large_max_size\t0.3\tMax large speckle size\n",
      "speckle_rating_penalty\t10\tPenalty to add to worst rating for noise\n",
      "xheight_penalty_subscripts\t0.125\tScore penalty (0.1 = 10%) added if there are subscripts or superscripts in a word, but it is otherwise OK.\n",
      "xheight_penalty_inconsistent\t0.25\tScore penalty (0.1 = 10%) added if an xheight is inconsistent.\n",
      "segment_penalty_dict_frequent_word\t1\tScore multiplier for word matches which have good case and are frequent in the given language (lower is better).\n",
      "segment_penalty_dict_case_ok\t1.1\tScore multiplier for word matches that have good case (lower is better).\n",
      "segment_penalty_dict_case_bad\t1.3125\tDefault score multiplier for word matches, which may have case issues (lower is better).\n",
      "segment_penalty_dict_nonword\t1.25\tScore multiplier for glyph fragment segmentations which do not match a dictionary word (lower is better).\n",
      "segment_penalty_garbage\t1.5\tScore multiplier for poorly cased strings that are not in the dictionary and generally look like garbage (lower is better).\n",
      "certainty_scale\t20\tCertainty scaling factor\n",
      "stopper_nondict_certainty_base\t-2.5\tCertainty threshold for non-dict words\n",
      "stopper_phase2_certainty_rejection_offset\t1\tReject certainty offset\n",
      "stopper_certainty_per_char\t-0.5\tCertainty to add for each dict char above small word size.\n",
      "stopper_allowable_character_badness\t3\tMax certainty variation allowed in a word (in sigma)\n",
      "doc_dict_pending_threshold\t0\tWorst certainty for using pending dictionary\n",
      "doc_dict_certainty_threshold\t-2.25\tWorst certainty for words that can be inserted into the document dictionary\n",
      "tessedit_certainty_threshold\t-2.25\tGood blob limit\n",
      "chop_split_dist_knob\t0.5\tSplit length adjustment\n",
      "chop_overlap_knob\t0.9\tSplit overlap adjustment\n",
      "chop_center_knob\t0.15\tSplit center adjustment\n",
      "chop_sharpness_knob\t0.06\tSplit sharpness adjustment\n",
      "chop_width_change_knob\t5\tWidth change adjustment\n",
      "chop_ok_split\t100\tOK split limit\n",
      "chop_good_split\t50\tGood split limit\n",
      "segsearch_max_char_wh_ratio\t2\tMaximum character width-to-height ratio\n",
      "language_model_ngram_small_prob\t1e-06\tTo avoid overly small denominators use this as the floor of the probability returned by the ngram model.\n",
      "language_model_ngram_nonmatch_score\t-40\tAverage classifier score of a non-matching unichar.\n",
      "language_model_ngram_scale_factor\t0.03\tStrength of the character ngram model relative to the character classifier \n",
      "language_model_ngram_rating_factor\t16\tFactor to bring log-probs into the same range as ratings when multiplied by outline length \n",
      "language_model_penalty_non_freq_dict_word\t0.1\tPenalty for words not in the frequent word dictionary\n",
      "language_model_penalty_non_dict_word\t0.15\tPenalty for non-dictionary words\n",
      "language_model_penalty_punc\t0.2\tPenalty for inconsistent punctuation\n",
      "language_model_penalty_case\t0.1\tPenalty for inconsistent case\n",
      "language_model_penalty_script\t0.5\tPenalty for inconsistent script\n",
      "language_model_penalty_chartype\t0.3\tPenalty for inconsistent character type\n",
      "language_model_penalty_font\t0\tPenalty for inconsistent font\n",
      "language_model_penalty_spacing\t0.05\tPenalty for inconsistent spacing\n",
      "language_model_penalty_increment\t0.01\tPenalty increment\n",
      "invert_threshold\t0.7\tFor lines with a mean confidence below this value, OCR is also tried with an inverted image\n",
      "thresholding_window_size\t0.33\tWindow size for measuring local statistics (to be multiplied by image DPI). This parameter is used by the Sauvola thresholding method\n",
      "thresholding_kfactor\t0.34\tFactor for reducing threshold due to variance. This parameter is used by the Sauvola thresholding method. Normal range: 0.2-0.5\n",
      "thresholding_tile_size\t0.33\tDesired tile size (to be multiplied by image DPI). This parameter is used by the LeptonicaOtsu thresholding method\n",
      "thresholding_smooth_kernel_size\t0\tSize of convolution kernel applied to threshold array (to be multiplied by image DPI). Use 0 for no smoothing. This parameter is used by the LeptonicaOtsu thresholding method\n",
      "thresholding_score_fraction\t0.1\tFraction of the max Otsu score. This parameter is used by the LeptonicaOtsu thresholding method. For standard Otsu use 0.0, otherwise 0.1 is recommended\n",
      "noise_cert_basechar\t-8\tHingepoint for base char certainty\n",
      "noise_cert_disjoint\t-1\tHingepoint for disjoint certainty\n",
      "noise_cert_punc\t-3\tThreshold for new punc char certainty\n",
      "noise_cert_factor\t0.375\tScaling on certainty diff from Hingepoint\n",
      "quality_rej_pc\t0.08\tgood_quality_doc lte rejection limit\n",
      "quality_blob_pc\t0\tgood_quality_doc gte good blobs limit\n",
      "quality_outline_pc\t1\tgood_quality_doc lte outline error limit\n",
      "quality_char_pc\t0.95\tgood_quality_doc gte good char limit\n",
      "test_pt_x\t100000\txcoord\n",
      "test_pt_y\t100000\tycoord\n",
      "tessedit_reject_doc_percent\t65\t%rej allowed before rej whole doc\n",
      "tessedit_reject_block_percent\t45\t%rej allowed before rej whole block\n",
      "tessedit_reject_row_percent\t40\t%rej allowed before rej whole row\n",
      "tessedit_whole_wd_rej_row_percent\t70\tNumber of row rejects in whole word rejects which prevents whole row rejection\n",
      "tessedit_good_doc_still_rowrej_wd\t1.1\trej good doc wd if more than this fraction rejected\n",
      "quality_rowrej_pc\t1.1\tgood_quality_doc gte good char limit\n",
      "crunch_terrible_rating\t80\tcrunch rating lt this\n",
      "crunch_poor_garbage_cert\t-9\tcrunch garbage cert lt this\n",
      "crunch_poor_garbage_rate\t60\tcrunch garbage rating lt this\n",
      "crunch_pot_poor_rate\t40\tPOTENTIAL crunch rating lt this\n",
      "crunch_pot_poor_cert\t-8\tPOTENTIAL crunch cert lt this\n",
      "crunch_del_rating\t60\tPOTENTIAL crunch rating lt this\n",
      "crunch_del_cert\t-10\tPOTENTIAL crunch cert lt this\n",
      "crunch_del_min_ht\t0.7\tDel if word ht lt xht x this\n",
      "crunch_del_max_ht\t3\tDel if word ht gt xht x this\n",
      "crunch_del_min_width\t3\tDel if word width lt xht x this\n",
      "crunch_del_high_word\t1.5\tDel if word gt xht x this above bl\n",
      "crunch_del_low_word\t0.5\tDel if word gt xht x this below bl\n",
      "crunch_small_outlines_size\t0.6\tSmall if lt xht x this\n",
      "fixsp_small_outlines_size\t0.28\tSmall if lt xht x this\n",
      "superscript_worse_certainty\t2\tHow many times worse certainty does a superscript position glyph need to be for us to try classifying it as a char with a different baseline?\n",
      "superscript_bettered_certainty\t0.97\tWhat reduction in badness do we think sufficient to choose a superscript over what we'd thought.  For example, a value of 0.6 means we want to reduce badness of certainty by at least 40%\n",
      "superscript_scaledown_ratio\t0.4\tA superscript scaled down more than this is unbelievably small.  For example, 0.3 means we expect the font size to be no smaller than 30% of the text line font size.\n",
      "subscript_max_y_top\t0.5\tMaximum top of a character measured as a multiple of x-height above the baseline for us to reconsider whether it's a subscript.\n",
      "superscript_min_y_bottom\t0.3\tMinimum bottom of a character measured as a multiple of x-height above the baseline for us to reconsider whether it's a superscript.\n",
      "suspect_rating_per_ch\t999.9\tDon't touch bad rating limit\n",
      "suspect_accept_rating\t-999.9\tAccept good rating limit\n",
      "tessedit_lower_flip_hyphen\t1.5\tAspect ratio dot/hyphen test\n",
      "tessedit_upper_flip_hyphen\t1.8\tAspect ratio dot/hyphen test\n",
      "rej_whole_of_mostly_reject_word_fract\t0.85\tif >this fract\n",
      "min_orientation_margin\t7\tMin acceptable orientation margin\n",
      "textord_tabfind_vertical_text_ratio\t0.5\tFraction of textlines deemed vertical to use vertical page mode\n",
      "textord_tabfind_aligned_gap_fraction\t0.75\tFraction of height used as a minimum gap for aligned blobs.\n",
      "lstm_rating_coefficient\t5\tSets the rating coefficient for the lstm choices. The smaller the coefficient, the better are the ratings for each choice and less information is lost due to the cut off at 0. The standard value is 5\n",
      "tosp_old_sp_kn_th_factor\t2\tFactor for defining space threshold in terms of space and kern sizes\n",
      "tosp_threshold_bias1\t0\thow far between kern and space?\n",
      "tosp_threshold_bias2\t0\thow far between kern and space?\n",
      "tosp_narrow_fraction\t0.3\tFract of xheight for narrow\n",
      "tosp_narrow_aspect_ratio\t0.48\tnarrow if w/h less than this\n",
      "tosp_wide_fraction\t0.52\tFract of xheight for wide\n",
      "tosp_wide_aspect_ratio\t0\twide if w/h less than this\n",
      "tosp_fuzzy_space_factor\t0.6\tFract of xheight for fuzz sp\n",
      "tosp_fuzzy_space_factor1\t0.5\tFract of xheight for fuzz sp\n",
      "tosp_fuzzy_space_factor2\t0.72\tFract of xheight for fuzz sp\n",
      "tosp_gap_factor\t0.83\tgap ratio to flip sp->kern\n",
      "tosp_kern_gap_factor1\t2\tgap ratio to flip kern->sp\n",
      "tosp_kern_gap_factor2\t1.3\tgap ratio to flip kern->sp\n",
      "tosp_kern_gap_factor3\t2.5\tgap ratio to flip kern->sp\n",
      "tosp_ignore_big_gaps\t-1\txht multiplier\n",
      "tosp_ignore_very_big_gaps\t3.5\txht multiplier\n",
      "tosp_rep_space\t1.6\trep gap multiplier for space\n",
      "tosp_enough_small_gaps\t0.65\tFract of kerns reqd for isolated row stats\n",
      "tosp_table_kn_sp_ratio\t2.25\tMin difference of kn & sp in table\n",
      "tosp_table_xht_sp_ratio\t0.33\tExpect spaces bigger than this\n",
      "tosp_table_fuzzy_kn_sp_ratio\t3\tFuzzy if less than this\n",
      "tosp_fuzzy_kn_fraction\t0.5\tNew fuzzy kn alg\n",
      "tosp_fuzzy_sp_fraction\t0.5\tNew fuzzy sp alg\n",
      "tosp_min_sane_kn_sp\t1.5\tDon't trust spaces less than this time kn\n",
      "tosp_init_guess_kn_mult\t2.2\tThresh guess - mult kn by this\n",
      "tosp_init_guess_xht_mult\t0.28\tThresh guess - mult xht by this\n",
      "tosp_max_sane_kn_thresh\t5\tMultiplier on kn to limit thresh\n",
      "tosp_flip_caution\t0\tDon't autoflip kn to sp when large separation\n",
      "tosp_large_kerning\t0.19\tLimit use of xht gap with large kns\n",
      "tosp_dont_fool_with_small_kerns\t-1\tLimit use of xht gap with odd small kns\n",
      "tosp_near_lh_edge\t0\tDon't reduce box if the top left is non blank\n",
      "tosp_silly_kn_sp_gap\t0.2\tDon't let sp minus kn get too small\n",
      "tosp_pass_wide_fuzz_sp_to_context\t0.75\tHow wide fuzzies need context\n",
      "textord_noise_area_ratio\t0.7\tFraction of bounding box for noise\n",
      "textord_initialx_ile\t0.75\tIle of sizes for xheight guess\n",
      "textord_initialasc_ile\t0.9\tIle of sizes for xheight guess\n",
      "textord_noise_sizelimit\t0.5\tFraction of x for big t count\n",
      "textord_noise_normratio\t2\tDot to norm ratio for deletion\n",
      "textord_noise_syfract\t0.2\txh fract height error for norm blobs\n",
      "textord_noise_sxfract\t0.4\txh fract width error for norm blobs\n",
      "textord_noise_hfract\t0.015625\tHeight fraction to discard outlines as speckle noise\n",
      "textord_noise_rowratio\t6\tDot to norm ratio for deletion\n",
      "textord_blshift_maxshift\t0\tMax baseline shift\n",
      "textord_blshift_xfraction\t9.99\tMin size of baseline shift\n"
     ]
    }
   ],
   "source": [
    "!tesseract --print-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f7e247f-c240-4992-b919-4312cbc896ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['eng', 'osd']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyt.get_languages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ded891b-c7bd-43eb-80f3-4313dbc2f8c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "e2660ff2-adfd-496b-baa6-21976df09b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "img1 = cv2.imread(r\"E:\\PyImage_ComputerVision\\OCR\\Working Data\\InputData\\Image1.png\")\n",
    "stvjb = cv2.imread(r\"E:\\PyImage_ComputerVision\\OCR\\Working Data\\InputData\\steve_jobs.png\")\n",
    "foods = cv2.imread(r\"E:\\PyImage_ComputerVision\\OCR\\Working Data\\InputData\\whole_foods.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "71e7d5d9-7d4a-4148-812e-3845af5abcd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(298, 626, 3)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f8c25f5d-ca3f-41f8-8374-2c8b2114dd82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(459, 800, 3)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stvjb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "25befe3b-e549-40fb-b323-c903f83b1f9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3264, 2448, 3)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "foods.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3f613780-aed2-40aa-97e6-53ca9d3ca8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "img1_rev = 255 - img1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "23374932-de52-4444-b1af-70b7bbcb1435",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imshow(\"Image1\",img1)\n",
    "cv2.imshow(\"reverse Image 1\",img1_rev)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fa7c0902-b304-4073-8d9d-64475c513484",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = pyt.image_to_string(img1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a8594c4f-8c89-4ac2-805f-164240fa60a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyImageSearch\n",
      "PO Box 17598 #17900\n",
      "Baltimore, MD 21297\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "335cbc48-abf6-43e1-ba63-d128fe6c1ba4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'PyImageSearch\\nPO Box 17598 #17900\\nBaltimore, MD 21297\\n'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3e6f3b69-717c-4e2f-8da3-0887e2b7371c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'level': [1, 2, 3, 4, 5, 4, 5, 5, 5, 5, 4, 5, 5, 5],\n",
       " 'page_num': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       " 'block_num': [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       " 'par_num': [0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       " 'line_num': [0, 0, 0, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3],\n",
       " 'word_num': [0, 0, 0, 0, 1, 0, 1, 2, 3, 4, 0, 1, 2, 3],\n",
       " 'left': [0, 27, 27, 27, 27, 27, 27, 123, 250, 412, 27, 27, 304, 423],\n",
       " 'top': [0, 33, 33, 33, 33, 121, 121, 122, 121, 121, 209, 209, 210, 209],\n",
       " 'width': [626,\n",
       "  573,\n",
       "  573,\n",
       "  393,\n",
       "  393,\n",
       "  573,\n",
       "  81,\n",
       "  105,\n",
       "  147,\n",
       "  188,\n",
       "  549,\n",
       "  263,\n",
       "  102,\n",
       "  153],\n",
       " 'height': [298, 232, 232, 62, 62, 48, 48, 46, 47, 47, 56, 56, 46, 47],\n",
       " 'conf': [-1, -1, -1, -1, 76, -1, 95, 95, 95, 96, -1, 96, 96, 96],\n",
       " 'text': ['',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  'PyImageSearch',\n",
       "  '',\n",
       "  'PO',\n",
       "  'Box',\n",
       "  '17598',\n",
       "  '#17900',\n",
       "  '',\n",
       "  'Baltimore,',\n",
       "  'MD',\n",
       "  '21297']}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pyt.image_to_data(img1,output_type=pyt.Output.DICT)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "03d2ba50-9434-4014-b990-22947201ba88",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(results[\"text\"])):\n",
    "    out = img1.copy()\n",
    "    conf = results[\"conf\"][i]\n",
    "    x = results[\"left\"][i]\n",
    "    y = results[\"top\"][i]\n",
    "    w = results[\"width\"][i]\n",
    "    h = results[\"height\"][i]\n",
    "    text = results[\"text\"][i]\n",
    "    if True:\n",
    "        cv2.rectangle(out,(x,y),(x+w,y+h),(0,255,0),1)\n",
    "        cv2.putText(out,text,(x,y),cv2.FONT_HERSHEY_SIMPLEX,1,(0,0,0),1)\n",
    "        cv2.imshow(\"Out\",out)\n",
    "        cv2.waitKey(0)\n",
    "        cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bc6db94e-e8ad-4701-bdce-9e6081d3f7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "boxes = pyt.image_to_boxes(img1,output_type=pyt.Output.DICT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0c60687a-05e0-4d55-8bef-96c1e6fe6fad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['char', 'left', 'bottom', 'right', 'top', 'page'])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boxes.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9c4533ce-5518-4cfc-a56d-d7231dd9656f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'char': ['P',\n",
       "  'y',\n",
       "  'I',\n",
       "  'm',\n",
       "  'a',\n",
       "  'g',\n",
       "  'e',\n",
       "  'S',\n",
       "  'e',\n",
       "  'a',\n",
       "  'r',\n",
       "  'c',\n",
       "  'h',\n",
       "  'P',\n",
       "  'O',\n",
       "  'B',\n",
       "  'o',\n",
       "  'x',\n",
       "  '1',\n",
       "  '7',\n",
       "  '5',\n",
       "  '9',\n",
       "  '8',\n",
       "  '#',\n",
       "  '1',\n",
       "  '7',\n",
       "  '9',\n",
       "  '0',\n",
       "  '0',\n",
       "  'B',\n",
       "  'a',\n",
       "  'l',\n",
       "  't',\n",
       "  'i',\n",
       "  'm',\n",
       "  'o',\n",
       "  'r',\n",
       "  'e',\n",
       "  ',',\n",
       "  'M',\n",
       "  'D',\n",
       "  '2',\n",
       "  '1',\n",
       "  '2',\n",
       "  '9',\n",
       "  '7'],\n",
       " 'left': [27,\n",
       "  63,\n",
       "  94,\n",
       "  94,\n",
       "  166,\n",
       "  192,\n",
       "  224,\n",
       "  252,\n",
       "  286,\n",
       "  301,\n",
       "  315,\n",
       "  361,\n",
       "  388,\n",
       "  27,\n",
       "  64,\n",
       "  123,\n",
       "  153,\n",
       "  167,\n",
       "  250,\n",
       "  275,\n",
       "  306,\n",
       "  338,\n",
       "  371,\n",
       "  412,\n",
       "  452,\n",
       "  477,\n",
       "  508,\n",
       "  539,\n",
       "  570,\n",
       "  27,\n",
       "  55,\n",
       "  72,\n",
       "  114,\n",
       "  135,\n",
       "  130,\n",
       "  197,\n",
       "  227,\n",
       "  248,\n",
       "  281,\n",
       "  304,\n",
       "  364,\n",
       "  423,\n",
       "  460,\n",
       "  485,\n",
       "  517,\n",
       "  548],\n",
       " 'bottom': [218,\n",
       "  203,\n",
       "  203,\n",
       "  218,\n",
       "  218,\n",
       "  203,\n",
       "  218,\n",
       "  217,\n",
       "  218,\n",
       "  203,\n",
       "  218,\n",
       "  218,\n",
       "  218,\n",
       "  130,\n",
       "  129,\n",
       "  130,\n",
       "  130,\n",
       "  130,\n",
       "  130,\n",
       "  130,\n",
       "  130,\n",
       "  130,\n",
       "  130,\n",
       "  130,\n",
       "  130,\n",
       "  130,\n",
       "  130,\n",
       "  130,\n",
       "  130,\n",
       "  42,\n",
       "  33,\n",
       "  42,\n",
       "  42,\n",
       "  33,\n",
       "  42,\n",
       "  42,\n",
       "  42,\n",
       "  42,\n",
       "  33,\n",
       "  42,\n",
       "  42,\n",
       "  42,\n",
       "  42,\n",
       "  42,\n",
       "  42,\n",
       "  42],\n",
       " 'right': [63,\n",
       "  94,\n",
       "  124,\n",
       "  166,\n",
       "  193,\n",
       "  222,\n",
       "  250,\n",
       "  282,\n",
       "  312,\n",
       "  325,\n",
       "  362,\n",
       "  386,\n",
       "  420,\n",
       "  63,\n",
       "  108,\n",
       "  162,\n",
       "  183,\n",
       "  228,\n",
       "  269,\n",
       "  303,\n",
       "  333,\n",
       "  367,\n",
       "  397,\n",
       "  446,\n",
       "  471,\n",
       "  505,\n",
       "  537,\n",
       "  569,\n",
       "  600,\n",
       "  66,\n",
       "  82,\n",
       "  114,\n",
       "  131,\n",
       "  162,\n",
       "  198,\n",
       "  227,\n",
       "  250,\n",
       "  274,\n",
       "  290,\n",
       "  406,\n",
       "  397,\n",
       "  453,\n",
       "  479,\n",
       "  515,\n",
       "  546,\n",
       "  576],\n",
       " 'top': [264,\n",
       "  249,\n",
       "  265,\n",
       "  264,\n",
       "  249,\n",
       "  249,\n",
       "  249,\n",
       "  265,\n",
       "  249,\n",
       "  265,\n",
       "  249,\n",
       "  249,\n",
       "  264,\n",
       "  176,\n",
       "  177,\n",
       "  176,\n",
       "  176,\n",
       "  161,\n",
       "  177,\n",
       "  176,\n",
       "  177,\n",
       "  177,\n",
       "  177,\n",
       "  176,\n",
       "  177,\n",
       "  176,\n",
       "  177,\n",
       "  177,\n",
       "  177,\n",
       "  88,\n",
       "  89,\n",
       "  89,\n",
       "  81,\n",
       "  89,\n",
       "  89,\n",
       "  73,\n",
       "  73,\n",
       "  73,\n",
       "  49,\n",
       "  88,\n",
       "  88,\n",
       "  89,\n",
       "  89,\n",
       "  89,\n",
       "  89,\n",
       "  88],\n",
       " 'page': [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0]}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "42ea14da-b630-4377-bb33-3872bd50294d",
   "metadata": {},
   "outputs": [],
   "source": [
    "h,w,_ = img1.shape\n",
    "for i in range(len(boxes[\"char\"])):\n",
    "    out = img1.copy()\n",
    "    char = boxes[\"char\"][i]\n",
    "    x1 = boxes[\"left\"][i]\n",
    "    y1 = boxes[\"top\"][i]\n",
    "    x2 = boxes[\"right\"][i]\n",
    "    y2 = boxes[\"bottom\"][i]\n",
    "    cv2.rectangle(out,(x1,h-y1),(x2,h-y2),(0,255,0),1)\n",
    "    cv2.putText(out,char,(x1,h-y1-10),cv2.FONT_HERSHEY_SIMPLEX,1,(0,0,0),1)\n",
    "    cv2.imshow(\"OUT\",out)\n",
    "    key = cv2.waitKey(0)\n",
    "    if key == ord('q'):\n",
    "        break\n",
    "    cv2.destroyAllWindows()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "3d780054-4b36-4ffc-b597-1ac67574760d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = boxes[\"left\"][3]\n",
    "x2 = boxes[\"right\"][3]\n",
    "y1 = boxes[\"top\"][3]\n",
    "y2 = boxes[\"bottom\"][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "7357c9ee-0ca6-4fa7-9575-219d18ad4012",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(94,)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "188821ed-5474-484b-b726-fafb8438951b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "166"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "0d652894-d3a2-4169-842d-53ea32ac7843",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "264"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "4b0ba84e-5075-42a9-8a1f-ae3425aa4360",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "218"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1565aeef-bfb2-4841-b2e5-329c69c9902d",
   "metadata": {},
   "outputs": [],
   "source": [
    "str = pyt.image_to_boxes(img1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a3a83c71-d3e8-469f-b74d-b7df1d9ec390",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'P 27 218 63 264 0\\ny 63 203 94 249 0\\nI 94 203 124 265 0\\nm 94 218 166 264 0\\na 166 218 193 249 0\\ng 192 203 222 249 0\\ne 224 218 250 249 0\\nS 252 217 282 265 0\\ne 286 218 312 249 0\\na 301 203 325 265 0\\nr 315 218 362 249 0\\nc 361 218 386 249 0\\nh 388 218 420 264 0\\nP 27 130 63 176 0\\nO 64 129 108 177 0\\nB 123 130 162 176 0\\no 153 130 183 176 0\\nx 167 130 228 161 0\\n1 250 130 269 177 0\\n7 275 130 303 176 0\\n5 306 130 333 177 0\\n9 338 130 367 177 0\\n8 371 130 397 177 0\\n# 412 130 446 176 0\\n1 452 130 471 177 0\\n7 477 130 505 176 0\\n9 508 130 537 177 0\\n0 539 130 569 177 0\\n0 570 130 600 177 0\\nB 27 42 66 88 0\\na 55 33 82 89 0\\nl 72 42 114 89 0\\nt 114 42 131 81 0\\ni 135 33 162 89 0\\nm 130 42 198 89 0\\no 197 42 227 73 0\\nr 227 42 250 73 0\\ne 248 42 274 73 0\\n, 281 33 290 49 0\\nM 304 42 406 88 0\\nD 364 42 397 88 0\\n2 423 42 453 89 0\\n1 460 42 479 89 0\\n2 485 42 515 89 0\\n9 517 42 546 89 0\\n7 548 42 576 88 0\\n'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622572de-342a-4193-ac76-826a1163f1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "left =63=x\n",
    "bottom=203=y\n",
    "right =94=w\n",
    "top=249=h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ed82584e-7a24-4c3d-b7de-0ac9be11b700",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(298, 626, 3)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "de851870-4f33-46db-9fe8-bb5d245c55fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27, 33, 420, 95, '')"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = results[\"left\"][3] \n",
    "y = results[\"top\"][3]\n",
    "w = results[\"width\"][3]\n",
    "h = results[\"height\"][3]\n",
    "text = results[\"text\"][3]\n",
    "x,y,x+w,y+h,text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "8a5da19e-9afa-40de-bd7f-da4ecd606758",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27, 33, 420, 95, 'PyImageSearch')"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = results[\"left\"][4] \n",
    "y = results[\"top\"][4]\n",
    "w = results[\"width\"][4]\n",
    "h = results[\"height\"][4]\n",
    "text = results[\"text\"][4]\n",
    "x,y,x+w,y+h,text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "a853f761-5a73-4f06-b0aa-49c754e462c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(94, 34, 166, 80)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1,img1.shape[0]-y1,x2,img1.shape[0]-y2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "c889d9a8-8a12-44ef-8cce-126c9881e308",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(94, 264, 166, 218)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1,y1,x2,y2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6cfab97-f4d0-47bd-8d9f-db2e6a94474f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyt.image_to_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bbcbb2b-c7e7-412b-aa4f-eed4197ce5b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "0c4448a1-303b-4496-989e-4a4509e3644d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "94-63"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "013799ca-eecd-4771-b5d9-2b590d45644c",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = pyt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "450326b6-97dd-4696-a4e6-2b5602b9e4cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30cfbcd5-0d41-4e48-b713-c76fe77c9c84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dbca381-ee2d-403d-999d-99df1216073a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "4e840c96-2032-40bb-a14a-45206ff83beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt1 = pyt.image_to_string(img1_rev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "37623839-ccf8-49b8-a724-6dca1805ab61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'PyImageSearch\\nPO Box 17598 #17900\\nBaltimore, MD 21297\\n'"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "ddf02d7a-af63-489c-853e-02b70e4a5444",
   "metadata": {},
   "outputs": [],
   "source": [
    "results1 = pyt.image_to_data(img1_rev,output_type=pyt.Output.DICT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "7d7b41e4-6fe0-4425-9f80-852f2d73fcff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'level': [1, 2, 3, 4, 5, 4, 5, 5, 5, 5, 4, 5, 5, 5],\n",
       " 'page_num': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       " 'block_num': [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       " 'par_num': [0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       " 'line_num': [0, 0, 0, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3],\n",
       " 'word_num': [0, 0, 0, 0, 1, 0, 1, 2, 3, 4, 0, 1, 2, 3],\n",
       " 'left': [0, 27, 27, 27, 27, 27, 27, 123, 250, 412, 27, 27, 304, 423],\n",
       " 'top': [0, 33, 33, 33, 33, 121, 121, 122, 121, 121, 209, 209, 210, 209],\n",
       " 'width': [626,\n",
       "  573,\n",
       "  573,\n",
       "  393,\n",
       "  393,\n",
       "  573,\n",
       "  81,\n",
       "  105,\n",
       "  147,\n",
       "  188,\n",
       "  549,\n",
       "  263,\n",
       "  102,\n",
       "  153],\n",
       " 'height': [298, 232, 232, 62, 62, 48, 48, 46, 47, 47, 56, 56, 46, 47],\n",
       " 'conf': [-1, -1, -1, -1, 72, -1, 95, 95, 96, 96, -1, 96, 96, 96],\n",
       " 'text': ['',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  'PyImageSearch',\n",
       "  '',\n",
       "  'PO',\n",
       "  'Box',\n",
       "  '17598',\n",
       "  '#17900',\n",
       "  '',\n",
       "  'Baltimore,',\n",
       "  'MD',\n",
       "  '21297']}"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "5bae816f-79d6-4347-9cba-b9e5ae980e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(results1[\"text\"])):\n",
    "    out = img1_rev.copy()\n",
    "    conf = results1[\"conf\"][i]\n",
    "    x = results1[\"left\"][i]\n",
    "    y = results1[\"top\"][i]\n",
    "    w = results1[\"width\"][i]\n",
    "    h = results1[\"height\"][i]\n",
    "    text = results1[\"text\"][i]\n",
    "    if True:\n",
    "        cv2.rectangle(out,(x,y),(x+w,y+h),(0,255,0),1)\n",
    "        cv2.putText(out,text,(x,y),cv2.FONT_HERSHEY_SIMPLEX,1,(255,255,255),1)\n",
    "        cv2.imshow(\"Out\",out)\n",
    "        cv2.waitKey(0)\n",
    "        cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "22a484b4-222f-4172-92e9-fa497829f817",
   "metadata": {},
   "outputs": [],
   "source": [
    "boxes1 = pyt.image_to_boxes(img1_rev,output_type=pyt.Output.DICT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "10d2c198-e19e-469f-9104-ecd6eda22e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "h,w,_ = img1_rev.shape\n",
    "for i in range(len(boxes1[\"char\"])):\n",
    "    out = img1_rev.copy()\n",
    "    char = boxes1[\"char\"][i]\n",
    "    x1 = boxes1[\"left\"][i]\n",
    "    y1 = boxes1[\"top\"][i]\n",
    "    x2 = boxes1[\"right\"][i]\n",
    "    y2 = boxes1[\"bottom\"][i]\n",
    "    cv2.rectangle(out,(x1,h-y1),(x2,h-y2),(0,255,0),1)\n",
    "    cv2.putText(out,char,(x1,h-y1-10),cv2.FONT_HERSHEY_SIMPLEX,1,(255,255,255),1)\n",
    "    cv2.imshow(\"OUT\",out)\n",
    "    key = cv2.waitKey(0)\n",
    "    if key == ord('q'):\n",
    "        break\n",
    "    cv2.destroyAllWindows()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a75127-45e0-4c91-ace8-776f11159131",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "6974aa0e-802f-4979-9014-99fd61803a2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(298, 626, 3)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "82ca84a0-db59-4c62-901a-58d36c1a8c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "nw  = 128\n",
    "ar = nw/298"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "91a4720c-1081-42d9-8117-31b1dbf3946b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "268.8859060402684"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ar * 626"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "3113e2bb-215e-4e54-a537-9e88e5a09eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_rs = cv2.resize(img1,(124,int(ar*626)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "a82324e7-7785-4027-b274-75ca54374650",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(268, 124, 3)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_rs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "2fe2fb04-789d-4be3-86ef-3abe29b35c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt3 = pyt.image_to_string(img_rs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "9a7acebd-8809-44b6-8bee-57569fb9672c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Miwa\\n\\nHine M4\\n'"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "8a167f58-6202-472b-95e9-1e336e26957f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imshow(\"Image Resize\",img_rs)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "b46c1e74-643d-46ed-b6b7-2ce70c297faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "results3 = pyt.image_to_data(img_rs,output_type=pyt.Output.DICT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "ad5576b4-dadf-4500-b657-8ee626f69d10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'level': [1, 2, 3, 4, 5, 2, 3, 4, 5, 5],\n",
       " 'page_num': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       " 'block_num': [0, 1, 1, 1, 1, 2, 2, 2, 2, 2],\n",
       " 'par_num': [0, 0, 1, 1, 1, 0, 1, 1, 1, 1],\n",
       " 'line_num': [0, 0, 0, 1, 1, 0, 0, 1, 1, 1],\n",
       " 'word_num': [0, 0, 0, 0, 1, 0, 0, 0, 1, 2],\n",
       " 'left': [0, 5, 5, 5, 5, 5, 5, 5, 5, 62],\n",
       " 'top': [0, 30, 30, 30, 30, 188, 188, 188, 188, 188],\n",
       " 'width': [124, 78, 78, 78, 78, 109, 109, 109, 49, 52],\n",
       " 'height': [268, 55, 55, 55, 55, 43, 43, 43, 42, 43],\n",
       " 'conf': [-1, -1, -1, -1, 39, -1, -1, -1, 51, 26],\n",
       " 'text': ['', '', '', '', 'Miwa', '', '', '', 'Hine', 'M4']}"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3cb1a3-148a-41f5-b2d6-d45a0e1ba71a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875a6b97-0400-44c7-8765-6b8d9092d196",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "eb0678a5-0594-413a-aecf-79d4c967825a",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_rs = cv2.resize(img1,(264,264))\n",
    "cv2.imshow(\"image Resize\",img_rs)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "cc1ed624-4bd9-43a7-96f8-36300a587ad6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'PylmageSearch\\nPO Box 1759817900\\nBaltimore, MD 21297\\n'"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text4 = pyt.image_to_string(img_rs)\n",
    "text4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7279fac6-b55d-4112-81b1-10838e95f954",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e674f343-baa1-49a8-8555-7f44f748550a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "8b1b02c2-99a1-4b9c-85dc-2a6980ca764b",
   "metadata": {},
   "outputs": [],
   "source": [
    "nh = 1280\n",
    "ar = 1280/626"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "65108e1a-51e7-458e-a836-7a7167dacbe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_rs = cv2.resize(img1,(int(ar*298),nh))\n",
    "cv2.imshow(\"Resized Image\",img_rs)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "4408ef77-0d8e-408c-a466-ddccbb4b1429",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1280, 609, 3)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_rs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "e47cb1d4-2464-4145-93d7-ffd870cf6720",
   "metadata": {},
   "outputs": [],
   "source": [
    "text5 = pyt.image_to_string(img_rs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "8a3d845c-b208-40ce-ad51-95cfe33ddb3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "138b1deb-311b-483f-b7b7-3bc1f898e13c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'level': [1],\n",
       " 'page_num': [1],\n",
       " 'block_num': [0],\n",
       " 'par_num': [0],\n",
       " 'line_num': [0],\n",
       " 'word_num': [0],\n",
       " 'left': [0],\n",
       " 'top': [0],\n",
       " 'width': [609],\n",
       " 'height': [1280],\n",
       " 'conf': [-1],\n",
       " 'text': ['']}"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results5 = pyt.image_to_data(img_rs,output_type=pyt.Output.DICT)\n",
    "results5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab0c2e5-5e99-42d3-bbfc-c95dc0078811",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
